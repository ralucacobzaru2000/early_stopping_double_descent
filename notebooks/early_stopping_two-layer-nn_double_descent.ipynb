{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Double descent in two layer neural network\n",
    "This notebook contains the relevant code for the following figures in the paper \"*Early stopping in deep networks: Double descent and how to eliminate it*\":\n",
    "\n",
    "- Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import datetime\n",
    "import pathlib\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append('../code/')\n",
    "from linear_utils import linear_model\n",
    "from train_utils import save_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argument written in command line format\n",
    "cli_args = '--seed 12 --save-results --jacobian --risk-loss L1 -t 50000 -w 1 0.01 --lr 0.000001 0.00008 -d 50 -n 100 --hidden 250 --sigmas geo --s-range 1 4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A fully-connected ReLU network with one hidden layer, trained to predict y from x\n",
    "by minimizing the MSE loss.\n",
    "\"\"\"\n",
    "\n",
    "# get CLI parameters\n",
    "parser = argparse.ArgumentParser(description='CLI parameters for training')\n",
    "parser.add_argument('--root', type=str, default='', metavar='DIR',\n",
    "                    help='Root directory')\n",
    "parser.add_argument('-t', '--iterations', type=int, default=1e4, metavar='ITERATIONS',\n",
    "                    help='Iterations (default: 1e4)')\n",
    "parser.add_argument('-n', '--samples', type=int, default=100, metavar='N',\n",
    "                    help='Number of samples (default: 100)')\n",
    "parser.add_argument('--print-freq', type=int, default=100,\n",
    "                    help='CLI output printing frequency (default: 1000)')\n",
    "parser.add_argument('--gpu', type=int, default=None,\n",
    "                    help='Number of GPUS to use')\n",
    "parser.add_argument('--seed', type=int, default=None,\n",
    "                    help='Random seed')                        \n",
    "parser.add_argument('-d', '--dim', type=int, default=50, metavar='DIMENSION',\n",
    "                    help='Feature dimension (default: 50)')\n",
    "parser.add_argument('--hidden', type=int, default=200, metavar='DIMENSION',\n",
    "                    help='Hidden layer dimension (default: 200)')\n",
    "parser.add_argument('--sigmas', type=str, default=None,\n",
    "                    help='Sigmas')     \n",
    "parser.add_argument('-r','--s-range', nargs='*', type=float,\n",
    "                    help='Range for sigmas')\n",
    "parser.add_argument('-w','--scales', nargs='*', type=float,\n",
    "                    help='scale of the weights')\n",
    "parser.add_argument('--lr', type=float, default=1e-4, nargs='*', metavar='LR',\n",
    "                    help='learning rate (default: 1e-4)')              \n",
    "parser.add_argument('--normalized', action='store_true', default=False,\n",
    "                    help='normalize sample norm across features')\n",
    "parser.add_argument('--risk-loss', type=str, default='MSE', metavar='LOSS',\n",
    "                    help='Loss for validation')\n",
    "parser.add_argument('--jacobian', action='store_true', default=False,\n",
    "                    help='compute the SVD of the jacobian of the network')\n",
    "parser.add_argument('--save-results', action='store_true', default=False,\n",
    "                    help='Save the results for plots')\n",
    "parser.add_argument('--details', type=str, metavar='N',\n",
    "                    default='no_detail_given',\n",
    "                    help='details about the experimental setup')\n",
    "\n",
    "\n",
    "args = parser.parse_args(cli_args.split())\n",
    "\n",
    "# directories\n",
    "root = pathlib.Path(args.root) if args.root else pathlib.Path.cwd().parent\n",
    "\n",
    "current_date = str(datetime.datetime.today().strftime('%Y-%m-%d-%H-%M-%S'))\n",
    "args.outpath = (pathlib.Path.cwd().parent / 'results' / 'two_layer_nn' /  current_date)\n",
    "\n",
    "if args.save_results:\n",
    "    args.outpath.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "if args.seed is not None:\n",
    "    random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    \n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_out = 1      # dimension of y\n",
    "\n",
    "# sample training set from the linear model\n",
    "lin_model = linear_model(args.dim, sigma_noise=0.0, normalized=False, sigmas=args.sigmas, s_range=args.s_range)\n",
    "Xs, ys = lin_model.sample(args.samples)\n",
    "Xs = torch.Tensor(Xs).to(device)\n",
    "ys = torch.Tensor(ys.reshape((-1,1))).to(device)\n",
    "\n",
    "# sample the set for empirical risk calculation\n",
    "Xt, yt = lin_model.sample(args.samples)\n",
    "Xt = torch.Tensor(Xt).to(device)\n",
    "yt = torch.Tensor(yt.reshape((-1,1))).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss functions\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "risk_fn = torch.nn.L1Loss(reduction='mean') if args.risk_loss == 'L1' else loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jacobian_two_layer(X, y, model, crit):\n",
    "    \n",
    "    grads = []\n",
    "    for cx, cy in zip(X, y):\n",
    "\n",
    "        cur_grads = []\n",
    "        model.zero_grad()\n",
    "        co = model(cx)\n",
    "        co.backward(torch.ones(len(cy)))\n",
    "\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None and len(p.data.shape)>1:\n",
    "                cur_grads.append(p.grad.data.numpy().flatten())\n",
    "        grads.append(np.concatenate(cur_grads))\n",
    "    return np.array(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled stepsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two layer neural network in pytorch\n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(args.dim, args.hidden),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(args.hidden, d_out),\n",
    "        ).to(device)\n",
    "\n",
    "\n",
    "#### re-initialize the weights (regular initialization is too unstable)\n",
    "# if args.scales:\n",
    "#     i = 0\n",
    "#     with torch.no_grad():\n",
    "#         for m in model:\n",
    "#             if type(m) == torch.nn.Linear:\n",
    "#                 if i == 0:\n",
    "#                     m.weight.data.normal_(0, args.scales[0])\n",
    "#                 if i == 1:\n",
    "#                     m.weight.data.uniform_(-args.scales[1], args.scales[1])\n",
    "#                 i += 1\n",
    "                \n",
    "                \n",
    "# use kaiming initialization instead                \n",
    "if args.scales:\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for m in model:\n",
    "            if type(m) == torch.nn.Linear:\n",
    "                if i == 0:\n",
    "                    torch.nn.init.kaiming_normal_(m.weight, a=math.sqrt(5))\n",
    "                    m.weight.data = torch.mul(m.weight.data, args.scales[0])\n",
    "                if i == 1:\n",
    "                    torch.nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))\n",
    "                    m.weight.data = torch.mul(m.weight.data, args.scales[1])\n",
    "                i += 1\n",
    "                \n",
    "\n",
    "# use same learning rate for the two layers in case of a single learning rate or none.\n",
    "if isinstance(args.lr, float):\n",
    "    args.lr = [args.lr] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type WindowsPath is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24256\\2194766938.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_results\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0msave_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         np_save_file = args.outpath / ('two_layer_nn_jacobian_' + \n",
      "\u001b[1;32m~\\OneDrive\\Documents\\GitHub\\localcopy\\early_stopping_double_descent\\notebooks\\../code\\train_utils.py\u001b[0m in \u001b[0;36msave_config\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;31m# a debuggability cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 431\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    432\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[1;34m(dct, _current_indent_level)\u001b[0m\n\u001b[0;32m    403\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    436\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Circular reference detected\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 438\u001b[1;33m             \u001b[0mo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m--> 179\u001b[1;33m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[0;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type WindowsPath is not JSON serializable"
     ]
    }
   ],
   "source": [
    "# compute the Jacobian at initialization\n",
    "if args.jacobian:\n",
    "    J = get_jacobian_two_layer(Xs, ys, model, loss_fn)\n",
    "    uv, sv, vtv = np.linalg.svd(J)\n",
    "\n",
    "    v1 = []\n",
    "    v2 = []\n",
    "    for i in range(sv.shape[0]):\n",
    "        v1.append(np.linalg.norm(vtv[i,:][:np.prod([250, 50])]))\n",
    "        v2.append(np.linalg.norm(vtv[i,:][-np.prod([1, 250]):]))\n",
    "    v1 = np.array(v1)\n",
    "    v2 = np.array(v2)\n",
    "    vTrec = np.linalg.norm(np.stack((v1, v2)), axis=0)\n",
    "\n",
    "\n",
    "    if args.save_results:\n",
    "        save_config(args)\n",
    "\n",
    "        np_save_file = args.outpath / ('two_layer_nn_jacobian_' + \n",
    "                                       str(args.scales[0]).replace('.', '-') + '_' + \n",
    "                                       str(args.scales[1]).replace('.', '-') + \n",
    "                                       '.txt')\n",
    "\n",
    "        np.savetxt(np_save_file, \n",
    "                   np.column_stack((sv, \n",
    "                                    v1,\n",
    "                                    v2,\n",
    "                                    vTrec\n",
    "                                   )), \n",
    "                   header='t vw vv vT', \n",
    "                   comments='',\n",
    "                   newline='\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [{'w': 0.01, 'v': 1}, {'w': 1, 'v': 1}, {'w': 1, 'v': 0.01}]\n",
    "cmap = matplotlib.cm.get_cmap('viridis')\n",
    "colorList = [cmap(50/1000), cmap(350/1000), cmap(650/1000)]\n",
    "labelList = [r'$W$', r'$v$', r'$W + v$']\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "\n",
    "ax_list = [plt.subplot(111)]\n",
    "\n",
    "ax_list[0].scatter(sv, v1, \n",
    "                color=colorList[0], \n",
    "                label=labelList[0],\n",
    "                lw=4)\n",
    "ax_list[0].scatter(sv, v2, \n",
    "                color=colorList[1], \n",
    "                label=labelList[1],\n",
    "#                 ls='dashed',\n",
    "                lw=4)\n",
    "ax_list[0].scatter(sv, vTrec, \n",
    "                color=colorList[2], \n",
    "                label=labelList[2],\n",
    "#                 ls='dashed',\n",
    "                lw=4)\n",
    "    \n",
    "ax_list[-1].legend(loc=0, bbox_to_anchor=(1, 0.5), fontsize='x-large',\n",
    "                   frameon=True, fancybox=True, shadow=True, ncol=1)\n",
    "ax_list[0].set_ylabel(r'$\\Vert v \\Vert_2^2$')\n",
    "\n",
    "# for i, ax in enumerate(ax_list): ax.set_title(r'$w = $' + str(weights[i]['w']) + \n",
    "#                                               r';$v = $' + str(weights[i]['v']))\n",
    "for ax in ax_list: ax.set_xlabel(r'$\\sigma_i$')\n",
    "for ax in ax_list: ax.set_xscale('log')\n",
    "for ax in ax_list: ax.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network\n",
    "losses = []\n",
    "risks = []\n",
    "for t in range(int(args.iterations)):\n",
    "    y_pred = model(Xs)\n",
    "\n",
    "    loss = loss_fn(y_pred, ys)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if not t % args.print_freq:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        for param in model.parameters():\n",
    "            param.data -= args.lr[i] * param.grad\n",
    "            if not len(param.shape) > 1:\n",
    "                i += 1\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        yt_pred = model(Xt)\n",
    "        \n",
    "        risk = risk_fn(yt_pred, yt)\n",
    "        risks.append(risk.item())\n",
    "\n",
    "        if not t % args.print_freq:\n",
    "            print(t, risk.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same stepsize\n",
    "\n",
    "Train the network with the same stepsizes for both layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = torch.nn.Sequential(\n",
    "           torch.nn.Linear(args.dim, args.hidden),\n",
    "           torch.nn.ReLU(),\n",
    "           torch.nn.Linear(args.hidden, d_out),\n",
    "         ).to(device)      \n",
    "                \n",
    "# use kaiming initialization                \n",
    "if args.scales:\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for m in model2:\n",
    "            if type(m) == torch.nn.Linear:\n",
    "                if i == 0:\n",
    "                    torch.nn.init.kaiming_normal_(m.weight, a=math.sqrt(5))\n",
    "                    m.weight.data = torch.mul(m.weight.data, args.scales[0])\n",
    "                if i == 1:\n",
    "                    torch.nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))\n",
    "                    m.weight.data = torch.mul(m.weight.data, args.scales[1])\n",
    "                i += 1\n",
    "                \n",
    "\n",
    "# use same learning rate for the two layers\n",
    "if isinstance(args.lr, list):\n",
    "    stepsize = [max(args.lr)] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network\n",
    "losses_same = []\n",
    "risks_same = []\n",
    "for t in range(int(args.iterations)):\n",
    "    y_pred = model2(Xs)\n",
    "\n",
    "    loss = loss_fn(y_pred, ys)\n",
    "    losses_same.append(loss.item())\n",
    "\n",
    "    if not t % args.print_freq:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    model2.zero_grad()\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        for param in model2.parameters():\n",
    "            param.data -= stepsize[i] * param.grad\n",
    "            if not len(param.shape) > 1:\n",
    "                i += 1\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        yt_pred = model2(Xt)\n",
    "        \n",
    "        risk = risk_fn(yt_pred, yt)\n",
    "        risks_same.append(risk.item())\n",
    "\n",
    "        if not t % args.print_freq:\n",
    "            print(t, risk.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risks = np.array(risks)\n",
    "risks_same = np.array(risks_same)\n",
    "geo_samples = [int(i) for i in np.geomspace(1, len(risks)-1, num=700)]\n",
    "\n",
    "\n",
    "if args.save_results:\n",
    "    save_config(args)\n",
    "    \n",
    "    np_save_file = args.outpath / ('two_layer_nn_risk_same_stepsize_s' + \n",
    "                                   str(int(args.s_range[0])) + \n",
    "                                   str(int(args.s_range[1])) + \n",
    "                                   '.txt')\n",
    "\n",
    "    geo_samples = [int(i) for i in np.geomspace(1, len(risks)-1, num=700)]\n",
    "\n",
    "    np.savetxt(np_save_file, \n",
    "               np.column_stack((geo_samples, \n",
    "                                risks_same[geo_samples],\n",
    "                                risks[geo_samples]\n",
    "                               )), \n",
    "               header='t dd no-dd', \n",
    "               comments='',\n",
    "               newline='\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = matplotlib.cm.get_cmap('viridis')\n",
    "colorList = [cmap(50/1000), cmap(350/1000)]\n",
    "labelList = ['same stepsize', 'scaled stepsize']\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax = plt.subplot(111)\n",
    "ax.set_xscale('log')\n",
    "\n",
    "\n",
    "ax.plot(geo_samples, risks_same[geo_samples], \n",
    "        color=colorList[0], \n",
    "        label=labelList[0],\n",
    "        lw=4)\n",
    "ax.plot(geo_samples, risks[geo_samples], \n",
    "        color=colorList[1], \n",
    "        label=labelList[1],\n",
    "        lw=4)\n",
    "\n",
    "ax.legend(loc=1, bbox_to_anchor=(1, 1), fontsize='x-large',\n",
    "    frameon=False, fancybox=True, shadow=True, ncol=1)\n",
    "ax.set_ylabel('risk')\n",
    "ax.set_xlabel(r'$t$ iterations')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [{'w': 0.01, 'v': 1}, {'w': 1, 'v': 1}, {'w': 1, 'v': 0.01}]\n",
    "cmap = matplotlib.cm.get_cmap('viridis')\n",
    "colorList = [cmap(50/1000), cmap(350/1000)]\n",
    "labelList = ['same stepsize', 'scaled stepsize']\n",
    "labelList2 = [r'$\\Vert v_{i,W} \\Vert_2^2$', r'$\\Vert v_{i,v} \\Vert_2^2$']\n",
    "\n",
    "fig = plt.figure(figsize=(24,12))\n",
    "\n",
    "ax_list = [plt.subplot(231), plt.subplot(232), plt.subplot(233)]\n",
    "ax_list2 = [plt.subplot(234), plt.subplot(235), plt.subplot(236)]\n",
    "for i, wlist in enumerate(weights):\n",
    "    cur_w = wlist['w']\n",
    "    cur_v = wlist['v']\n",
    "    \n",
    "    cur_risks = np.loadtxt('../results/two_layer_nn/' + \n",
    "                           'w' + ''.join(str(cur_w).split('.')) + \n",
    "                           '_v' + ''.join(str(cur_v).split('.')) +\n",
    "                           '_lr80/risk.txt',\n",
    "                          skiprows=1)\n",
    "    \n",
    "    cur_jac = np.loadtxt('../results/two_layer_nn/' + \n",
    "                         'w' + ''.join(str(cur_w).split('.')) + \n",
    "                         '_v' + ''.join(str(cur_v).split('.')) +\n",
    "                         '_jacobian/' + \n",
    "                         'two_layer_nn_jacobian' + \n",
    "                         '_' + '-'.join(str(float(cur_w)).split('.')) + \n",
    "                         '_' + '-'.join(str(float(cur_v)).split('.')) + \n",
    "                         '.txt',\n",
    "                          skiprows=1)\n",
    "    \n",
    "    ax_list[i].plot(cur_risks[:,0], cur_risks[:,1], \n",
    "                    color=colorList[0], \n",
    "                    label=labelList[0],\n",
    "                    lw=4)\n",
    "    ax_list[i].plot(cur_risks[:,0], cur_risks[:,2], \n",
    "                    color=colorList[1], \n",
    "                    label=labelList[1],\n",
    "                    ls='dashed',\n",
    "                    lw=4)\n",
    "    \n",
    "    ax_list2[i].scatter(cur_jac[:,0], cur_jac[:,1], \n",
    "                color=colorList[0], \n",
    "                label=labelList2[0],\n",
    "                lw=4)\n",
    "    ax_list2[i].scatter(cur_jac[:,0], cur_jac[:,2], \n",
    "                color=colorList[1], \n",
    "                label=labelList2[1],\n",
    "                lw=4)\n",
    "    \n",
    "ax_list[-1].legend(loc=1, bbox_to_anchor=(1, 1), fontsize='x-large',\n",
    "                   frameon=False, fancybox=True, shadow=True, ncol=1)\n",
    "ax_list2[-1].legend(loc=1, bbox_to_anchor=(1, 0.85), fontsize='x-large',\n",
    "                   frameon=False, fancybox=True, shadow=True, ncol=1)\n",
    "ax_list[0].set_ylabel('risk')\n",
    "ax_list2[0].set_ylabel(r'$\\Vert \\cdot \\Vert_2^2$')\n",
    "\n",
    "for i, ax in enumerate(ax_list): ax.set_title(r'$w = $' + str(weights[i]['w']) + \n",
    "                                              r';$v = $' + str(weights[i]['v']))\n",
    "for ax in ax_list: ax.set_xlabel(r'$t$ iterations')\n",
    "for ax in ax_list2: ax.set_xlabel(r'$\\sigma_i$')\n",
    "for ax in ax_list: ax.set_xscale('log')\n",
    "for ax in ax_list2: ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
